{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdba9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Physical parameters\n",
    "m = 1.0    # Mass (kg)\n",
    "L = 1.0    # Length (m)\n",
    "b = 0.05   # Damping coefficient (kg/s)\n",
    "g = 9.81   # Gravitational acceleration (m/s^2)\n",
    "A = 1.0    # Amplitude of the forcing function\n",
    "omega = 1.0 # Driving frequency\n",
    "T = 20.0   # Total time (s)\n",
    "N_cp = 500 # Number of collocation points\n",
    "\n",
    "# Time discretization\n",
    "t = torch.linspace(0, T, N_cp).unsqueeze(1)\n",
    "\n",
    "# Initial conditions\n",
    "theta_0 = 0.5  # Initial angular displacement (rad)\n",
    "omega_0 = 0.0  # Initial angular velocity (rad/s)\n",
    "\n",
    "# Forcing function\n",
    "def forcing_function(t):\n",
    "    return A * torch.sin(omega * t)\n",
    "\n",
    "# PINN Vanilla Neural Network\n",
    "class VanillaPINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VanillaPINN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, t):\n",
    "        return self.net(t)\n",
    "\n",
    "# Stacked PINN Neural Network\n",
    "class StackedPINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StackedPINN, self).__init__()\n",
    "        self.stacked_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1, 100),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(100, 100),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(100, 2)\n",
    "            ) for _ in range(4)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, t):\n",
    "        out = self.stacked_layers[0](t)\n",
    "        for layer in self.stacked_layers[1:]:\n",
    "            out += layer(t)\n",
    "        return out\n",
    "\n",
    "# Physics-Informed Loss Function\n",
    "def loss_function(t, theta_pred, omega_pred):\n",
    "    # Compute the derivatives of theta and omega\n",
    "    theta_t = torch.autograd.grad(theta_pred, t, grad_outputs=torch.ones_like(theta_pred), create_graph=True)[0]\n",
    "    omega_t = torch.autograd.grad(omega_pred, t, grad_outputs=torch.ones_like(omega_pred), create_graph=True)[0]\n",
    "\n",
    "    # Compute the second derivative of theta (angular acceleration)\n",
    "    theta_tt = torch.autograd.grad(omega_t, t, grad_outputs=torch.ones_like(omega_t), create_graph=True)[0]\n",
    "\n",
    "    # Forcing function\n",
    "    F_t = forcing_function(t)\n",
    "\n",
    "    # Equation of motion: d^2theta/dt^2 + (b/m) * dtheta/dt + (g/L) * sin(theta) = F(t)\n",
    "    residual = theta_tt + (b / m) * omega_pred + (g / L) * torch.sin(theta_pred) - F_t\n",
    "\n",
    "    # Mean squared error loss\n",
    "    mse_residual = torch.mean(residual ** 2)\n",
    "\n",
    "    return mse_residual\n",
    "\n",
    "# Runge-Kutta (RK4) Integrator for Numerical Solution\n",
    "class ForcedPendulumODE(nn.Module):\n",
    "    def __init__(self, b, m, g, L):\n",
    "        super(ForcedPendulumODE, self).__init__()\n",
    "        self.b = b\n",
    "        self.m = m\n",
    "        self.g = g\n",
    "        self.L = L\n",
    "    \n",
    "    def forward(self, x, t=None):\n",
    "        theta = x[:, [0]]\n",
    "        omega = x[:, [1]]\n",
    "        dtheta_dt = omega\n",
    "        domega_dt = -self.b / self.m * omega - self.g / self.L * torch.sin(theta) + forcing_function(t)\n",
    "        return torch.cat([dtheta_dt, domega_dt], dim=-1)\n",
    "\n",
    "# Numerical RK4 Solution\n",
    "def runge_kutta_4th(ode, y0, t_eval):\n",
    "    y_t = y0\n",
    "    sol = [y_t]\n",
    "    for t in t_eval[1:]:\n",
    "        y_t = ode(y_t, t)\n",
    "        sol.append(y_t)\n",
    "    return torch.stack(sol).detach().numpy()\n",
    "\n",
    "# Initialize models, optimizers, and ODE solver\n",
    "vanilla_pinn = VanillaPINN()\n",
    "stacked_pinn = StackedPINN()\n",
    "pendulum_ode = ForcedPendulumODE(b, m, g, L)\n",
    "\n",
    "# Numerical RK4 integration\n",
    "y0 = torch.tensor([[theta_0, omega_0]])\n",
    "t_eval = torch.linspace(0, T, N_cp)\n",
    "sol_rk4 = runge_kutta_4th(pendulum_ode, y0, t_eval)\n",
    "\n",
    "# Training the models\n",
    "def train_model(model, epochs=10000, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_history = []\n",
    "    \n",
    "    t.requires_grad = True\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass: compute predicted theta and omega\n",
    "        outputs = model(t)\n",
    "        theta_pred = outputs[:, 0:1]\n",
    "        omega_pred = outputs[:, 1:2]\n",
    "\n",
    "        # Compute the physics-informed loss\n",
    "        loss = loss_function(t, theta_pred, omega_pred)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store the loss\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        # Print progress every 1000 epochs\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "    return model, loss_history\n",
    "\n",
    "# Train vanilla and stacked models\n",
    "vanilla_pinn, vanilla_loss = train_model(vanilla_pinn, epochs=5000)\n",
    "stacked_pinn, stacked_loss = train_model(stacked_pinn, epochs=5000)\n",
    "\n",
    "# Plot loss history\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(vanilla_loss, label='Vanilla PINN Loss')\n",
    "plt.plot(stacked_loss, label='Stacked PINN Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss History')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Predict using trained models\n",
    "theta_pred_vanilla, omega_pred_vanilla = vanilla_pinn(t).detach().numpy().T\n",
    "theta_pred_stacked, omega_pred_stacked = stacked_pinn(t).detach().numpy().T\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Angular Displacement\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(t.detach().numpy(), theta_pred_vanilla, label='Vanilla PINN $\\\\theta(t)$')\n",
    "plt.plot(t.detach().numpy(), theta_pred_stacked, label='Stacked PINN $\\\\theta(t)$')\n",
    "plt.plot(t_eval.detach().numpy(), sol_rk4[:,0,0], label='Numerical RK4 $\\\\theta(t)$', linestyle='dashed')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Angular Displacement [rad]')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Angular Velocity\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(t.detach().numpy(), omega_pred_vanilla, label='Vanilla PINN $\\\\omega(t)$')\n",
    "plt.plot(t.detach().numpy(), omega_pred_stacked, label='Stacked PINN $\\\\omega(t)$')\n",
    "plt.plot(t_eval.detach().numpy(), sol_rk4[:,0,1], label='Numerical RK4 $\\\\omega(t)$', linestyle='dashed')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Angular Velocity [rad/s]')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verify that the stacked PINN parameters are small (e.g., any regularization or attention mechanisms)\n",
    "for idx, param in enumerate(stacked_pinn.parameters()):\n",
    "    print(f\"Stacked PINN parameter {idx}: {param.data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4121bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Physical parameters\n",
    "m = 1.0    # Mass (kg)\n",
    "L = 1.0    # Length (m)\n",
    "b = 0.05   # Damping coefficient (kg/s)\n",
    "g = 9.81   # Gravitational acceleration (m/s^2)\n",
    "A = 1.0    # Amplitude of the forcing function\n",
    "omega = 1.0 # Driving frequency\n",
    "T = 20.0   # Total time (s)\n",
    "N_cp = 500 # Number of collocation points\n",
    "\n",
    "# Time discretization\n",
    "t = torch.linspace(0, T, N_cp).unsqueeze(1)\n",
    "\n",
    "# Initial conditions\n",
    "theta_0 = 0.5  # Initial angular displacement (rad)\n",
    "omega_0 = 0.0  # Initial angular velocity (rad/s)\n",
    "\n",
    "# Forcing function\n",
    "def forcing_function(t):\n",
    "    return A * torch.sin(omega * t)\n",
    "\n",
    "# Enhanced PINN Neural Network with energy conservation\n",
    "class EnhancedPINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnhancedPINN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, t):\n",
    "        return self.net(t)\n",
    "\n",
    "# Energy function for physical constraint\n",
    "def compute_total_energy(theta, omega):\n",
    "    # Kinetic energy: (1/2) m L^2 omega^2\n",
    "    kinetic_energy = 0.5 * m * L**2 * omega**2\n",
    "    \n",
    "    # Potential energy: m g L (1 - cos(theta))\n",
    "    potential_energy = m * g * L * (1 - torch.cos(theta))\n",
    "    \n",
    "    # Total mechanical energy\n",
    "    return kinetic_energy + potential_energy\n",
    "\n",
    "# Physics-Informed Loss Function with energy conservation\n",
    "def loss_function(t, theta_pred, omega_pred):\n",
    "    # Compute the derivatives of theta and omega\n",
    "    theta_t = torch.autograd.grad(theta_pred, t, grad_outputs=torch.ones_like(theta_pred), create_graph=True)[0]\n",
    "    omega_t = torch.autograd.grad(omega_pred, t, grad_outputs=torch.ones_like(omega_pred), create_graph=True)[0]\n",
    "\n",
    "    # Compute the second derivative of theta (angular acceleration)\n",
    "    theta_tt = torch.autograd.grad(omega_t, t, grad_outputs=torch.ones_like(omega_t), create_graph=True)[0]\n",
    "\n",
    "    # Forcing function\n",
    "    F_t = forcing_function(t)\n",
    "\n",
    "    # Equation of motion: d^2theta/dt^2 + (b/m) * dtheta/dt + (g/L) * sin(theta) = F(t)\n",
    "    residual = theta_tt + (b / m) * omega_pred + (g / L) * torch.sin(theta_pred) - F_t\n",
    "\n",
    "    # Mean squared error loss for the ODE residual\n",
    "    mse_residual = torch.mean(residual ** 2)\n",
    "\n",
    "    # Compute total energy at each point in time\n",
    "    energy_pred = compute_total_energy(theta_pred, omega_pred)\n",
    "\n",
    "    # Reference energy (initial energy)\n",
    "    initial_energy = compute_total_energy(torch.tensor(theta_0), torch.tensor(omega_0))\n",
    "\n",
    "    # Energy conservation loss: the energy should be close to the initial energy\n",
    "    energy_loss = torch.mean((energy_pred - initial_energy) ** 2)\n",
    "\n",
    "    # Total loss is a combination of the ODE residual and energy conservation loss\n",
    "    return mse_residual + energy_loss\n",
    "\n",
    "# Runge-Kutta (RK4) Integrator for Numerical Solution\n",
    "class ForcedPendulumODE(nn.Module):\n",
    "    def __init__(self, b, m, g, L):\n",
    "        super(ForcedPendulumODE, self).__init__()\n",
    "        self.b = b\n",
    "        self.m = m\n",
    "        self.g = g\n",
    "        self.L = L\n",
    "    \n",
    "    def forward(self, x, t=None):\n",
    "        theta = x[:, [0]]\n",
    "        omega = x[:, [1]]\n",
    "        dtheta_dt = omega\n",
    "        domega_dt = -self.b / self.m * omega - self.g / self.L * torch.sin(theta) + forcing_function(t)\n",
    "        return torch.cat([dtheta_dt, domega_dt], dim=-1)\n",
    "\n",
    "# Numerical RK4 Solution\n",
    "def runge_kutta_4th(ode, y0, t_eval):\n",
    "    y_t = y0\n",
    "    sol = [y_t]\n",
    "    for t in t_eval[1:]:\n",
    "        y_t = ode(y_t, t)\n",
    "        sol.append(y_t)\n",
    "    return torch.stack(sol).detach().numpy()\n",
    "\n",
    "# Initialize models, optimizers, and ODE solver\n",
    "enhanced_pinn = EnhancedPINN()\n",
    "pendulum_ode = ForcedPendulumODE(b, m, g, L)\n",
    "\n",
    "# Numerical RK4 integration\n",
    "y0 = torch.tensor([[theta_0, omega_0]])\n",
    "t_eval = torch.linspace(0, T, N_cp)\n",
    "sol_rk4 = runge_kutta_4th(pendulum_ode, y0, t_eval)\n",
    "\n",
    "# Training the model\n",
    "def train_model(model, epochs=10000, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_history = []\n",
    "    \n",
    "    t.requires_grad = True\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass: compute predicted theta and omega\n",
    "        outputs = model(t)\n",
    "        theta_pred = outputs[:, 0:1]\n",
    "        omega_pred = outputs[:, 1:2]\n",
    "\n",
    "        # Compute the physics-informed loss\n",
    "        loss = loss_function(t, theta_pred, omega_pred)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store the loss\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        # Print progress every 1000 epochs\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "    return model, loss_history\n",
    "\n",
    "# Train the enhanced model\n",
    "enhanced_pinn, enhanced_loss = train_model(enhanced_pinn, epochs=5000)\n",
    "\n",
    "# Plot loss history\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(enhanced_loss, label='Enhanced PINN Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss History (Enhanced PINN with Energy Conservation)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Predict using trained models\n",
    "theta_pred, omega_pred = enhanced_pinn(t).detach().numpy().T\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Angular Displacement\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(t.detach().numpy(), theta_pred, label='Enhanced PINN $\\\\theta(t)$')\n",
    "plt.plot(t_eval.detach().numpy(), sol_rk4[:,0,0], label='Numerical RK4 $\\\\theta(t)$', linestyle='dashed')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Angular Displacement [rad]')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Angular Velocity\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(t.detach().numpy(), omega_pred, label='Enhanced PINN $\\\\omega(t)$')\n",
    "plt.plot(t_eval.detach().numpy(), sol_rk4[:,0,1], label='Numerical RK4 $\\\\omega(t)$', linestyle='dashed')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Angular Velocity [rad/s]')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verify learned parameters\n",
    "for idx, param in enumerate(enhanced_pinn.parameters()):\n",
    "    print(f\"Enhanced PINN parameter {idx}: {param.data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a43ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
